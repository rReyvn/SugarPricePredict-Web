{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa70b6a",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c67c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn import tree\n",
    "from hijridate import Hijri, Gregorian\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633fbc7",
   "metadata": {},
   "source": [
    "# Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf63192",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "# Specify where datasets stored\n",
    "raw_dataset_dir = Path(\"../dataset/raw\")\n",
    "paths = sorted(raw_dataset_dir.glob(\"*.xlsx\"))\n",
    "\n",
    "# Check if datasets exists\n",
    "if not paths:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Tidak ditemukan dataset .xlsx pada {raw_dataset_dir.resolve()}\"\n",
    "    )\n",
    "\n",
    "# Print loaded dataset\n",
    "print(f\"Loaded Dataset :\")\n",
    "for path in paths:\n",
    "    name = path.stem\n",
    "    print(f\"{path.name}\")\n",
    "    # Read dataset, Drop unnecessary column, and Revise columns name\n",
    "    df_raw = (\n",
    "        pd.read_excel(path)\n",
    "        .drop(columns=[\"No\"])\n",
    "        .rename(columns={\"Komoditas (Rp)\": \"Province\"})\n",
    "    )\n",
    "    # Drop summary row\n",
    "    df_raw = df_raw[df_raw[\"Province\"] != \"Semua Provinsi\"].reset_index(drop=True)\n",
    "    raw_datasets[name] = df_raw.reset_index(drop=True)\n",
    "    # Print dataframe\n",
    "    print(df_raw.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b221aa",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c174761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where cleaned dataset stored\n",
    "clean_dataset_dir = Path(\"../dataset/clean\")\n",
    "clean_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cleaned_datasets: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "# Loop process to clean each raw dataset\n",
    "for name, df in raw_datasets.items():\n",
    "    # Select Date\n",
    "    date_cols = [c for c in df.columns if c not in [\"Province\"]]\n",
    "\n",
    "    # Change dataset format from wide to long\n",
    "    df_long = df.melt(\n",
    "        id_vars=[\"Province\"],\n",
    "        value_vars=date_cols,\n",
    "        var_name=\"Date\",\n",
    "        value_name=\"Price\",\n",
    "    )\n",
    "\n",
    "    # Clean date column by removing space and change date format\n",
    "    df_long[\"Date\"] = pd.to_datetime(\n",
    "        df_long[\"Date\"].str.replace(\" \", \"\").str.strip(),\n",
    "        format=\"%d/%m/%Y\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "\n",
    "    # Clean price column by replacing \"-\" with NaN, remove comma, and format to numeric\n",
    "    df_long[\"Price\"] = (\n",
    "        df_long[\"Price\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace(\"-\", np.nan)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "    )\n",
    "    df_long[\"Price\"] = pd.to_numeric(df_long[\"Price\"], errors=\"coerce\")\n",
    "\n",
    "    # Forward/Backward Fill Based Each Province\n",
    "    df_long = (\n",
    "        df_long.groupby(\"Province\", sort=False, group_keys=False)[[\"Date\", \"Price\", \"Province\"]]\n",
    "        .apply(\n",
    "            lambda g: (\n",
    "                g.set_index(\"Date\")\n",
    "                .reindex(\n",
    "                    pd.date_range(\n",
    "                        start=g[\"Date\"].min(),\n",
    "                        end=g[\"Date\"].max(),\n",
    "                        freq=\"D\",\n",
    "                    )\n",
    "                )\n",
    "                .assign(Province=lambda x: x[\"Province\"].ffill().bfill())\n",
    "                .assign(\n",
    "                    Price=lambda x: x[\"Price\"].ffill().bfill()\n",
    "                )\n",
    "                .rename_axis(\"Date\")\n",
    "                .reset_index()\n",
    "            )\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Sort by date and province\n",
    "    df_long = df_long.sort_values([\"Date\", \"Province\"]).reset_index(drop=True)\n",
    "\n",
    "    # Round price\n",
    "    df_long[\"Price\"] = df_long[\"Price\"].round().astype(int)\n",
    "\n",
    "    # Export cleaned dataframe\n",
    "    file_name = f\"Clean_{name}.xlsx\"\n",
    "    df_long.to_excel(clean_dataset_dir / file_name, index=False)\n",
    "\n",
    "    # Print preview cleaned dataframe\n",
    "    print(df_long.head(5))\n",
    "    print(\"...\")\n",
    "    print(df_long.tail(5))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Save to dictionary\n",
    "    cleaned_datasets[name] = df_long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17f091",
   "metadata": {},
   "source": [
    "# Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset_path = Path(\"../dataset/merged/merged_datasets.xlsx\")\n",
    "merged_dataset_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "frames: list[pd.DataFrame] = []\n",
    "\n",
    "for name, df_transform in cleaned_datasets.items():\n",
    "    new_df = df_transform.copy()\n",
    "    frames.append(new_df)\n",
    "\n",
    "# Merge all dataset\n",
    "df_merged = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Sort and drop duplicate each province and date\n",
    "# If there's duplicate data, get the last one\n",
    "df_merged = df_merged.drop_duplicates(\n",
    "    subset=[\"Province\", \"Date\"], keep=\"last\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Export merged dataframe\n",
    "df_merged.to_excel(merged_dataset_path)\n",
    "\n",
    "# Print preview merged dataframe\n",
    "print(df_merged.head(5))\n",
    "print(\"...\")\n",
    "print(df_merged.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52a323c",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if date is eid\n",
    "def eid_delta_days(ts: pd.Timestamp) -> int:\n",
    "    g = Gregorian(ts.year, ts.month, ts.day)\n",
    "    h = g.to_hijri()\n",
    "\n",
    "    # 1 Syawal = 10 Hijriah\n",
    "    eid_h = Hijri(h.year, 10, 1)\n",
    "    eid_g = eid_h.to_gregorian()\n",
    "\n",
    "    eid_date = pd.Timestamp(eid_g.year, eid_g.month, eid_g.day)\n",
    "    return (ts.normalize() - eid_date).days\n",
    "\n",
    "# Flag date based on eid 14 day window\n",
    "def eid_flags(ts: pd.Timestamp):\n",
    "    d = eid_delta_days(ts)\n",
    "    before = 1 if -7 <= d <= -1 else 0\n",
    "    day     = 1 if d == 0 else 0\n",
    "    after  = 1 if 1 <= d <= 6 else 0\n",
    "    return pd.Series([before, day, after],\n",
    "                     index=[\"before_eid\", \"eid\", \"after_eid\"])\n",
    "\n",
    "# Specify where transformed dataset stored\n",
    "transformed_dataset_path = Path(\"../dataset/transformed/transformed_datasets.xlsx\")\n",
    "transformed_dataset_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = df_merged.copy()\n",
    "\n",
    "# Data type normalization\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"Province\"] = df[\"Province\"].astype(\"string\")\n",
    "df[\"Price\"] = pd.to_numeric(df[\"Price\"], errors=\"coerce\")\n",
    "\n",
    "# Feature Engineering\n",
    "## 1) Province â†’ numeric (ID)\n",
    "# Calculate average price for each province\n",
    "province_mean_prices = df.groupby(\"Province\")[\"Price\"].mean().sort_values()\n",
    "# Create a mapping dictionary from province name to a rank (0 to 33)\n",
    "province_mapping = {province: i for i, province in enumerate(province_mean_prices.index)}\n",
    "df[\"Province_id\"] = df[\"Province\"].map(province_mapping)\n",
    "\n",
    "print(\"Province Mapping (Lowest to Highest Price):\")\n",
    "print(province_mapping)\n",
    "\n",
    "## 2) Lag features for each province\n",
    "for lag in [1, 14]:\n",
    "    df[f\"lag_{lag}\"] = df.groupby(\"Province\", group_keys=False)[\"Price\"].shift(lag)\n",
    "\n",
    "## 3) Moving holiday features\n",
    "df[[\"before_eid\", \"eid\", \"after_eid\"]] = df[\"Date\"].apply(eid_flags)\n",
    "\n",
    "## 4) Time based features\n",
    "df[\"month\"] = df[\"Date\"].dt.month\n",
    "df[\"year\"] = df[\"Date\"].dt.year\n",
    "\n",
    "# Drop row data with no lag features\n",
    "df_transform = df.dropna(subset=[\"lag_1\", \"lag_14\"]).reset_index(drop=True)\n",
    "\n",
    "# Export transformed dataframe\n",
    "df_transform.to_excel(transformed_dataset_path)\n",
    "\n",
    "# Print preview transformed dataframe\n",
    "print(\"\\n\")\n",
    "print(df_transform.head(5))\n",
    "print(\"...\")\n",
    "print(df_transform.tail(5))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4119d2",
   "metadata": {},
   "source": [
    "# Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b934967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where mined dataset stored\n",
    "mined_dataset_path = Path(\"../dataset/mined/rfr_datasets.xlsx\")\n",
    "mined_dataset_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_mining = df_transform.copy()\n",
    "\n",
    "# Configure Parameters, Train Size, etc\n",
    "RFR_PARAMS = dict(\n",
    "    n_estimators=60,\n",
    "    max_depth=None,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Amount of data used for training\n",
    "train_size = 0.9\n",
    "\n",
    "# Specify Feature Columns and Target Column\n",
    "FEATURE_COLS = [\"Province_id\", \"lag_1\", \"lag_14\", \"before_eid\", \"eid\", \"after_eid\", \"month\", \"year\"]\n",
    "TARGET_COL = \"Price\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3239e1a8",
   "metadata": {},
   "source": [
    "## Creating Model for Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86539e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify X for features and y for target\n",
    "X = df_mining[FEATURE_COLS]\n",
    "y = df_mining[TARGET_COL]\n",
    "\n",
    "# Time-based train-test split\n",
    "split_index = int(len(df_mining) * train_size)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "model = RandomForestRegressor(**RFR_PARAMS)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dbd2e1",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63425c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278d318",
   "metadata": {},
   "source": [
    "# Knowledge Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29511e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"=== Random Forest Regression ===\")\n",
    "print(f\"Train Data : {train_size*100}%\")\n",
    "print(f\"RMSE : {rmse:,.2f}\")\n",
    "print(f\"MAPE : {mape:.2f}%\")\n",
    "\n",
    "# Save prediction results for analysis\n",
    "mined_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": df_mining.iloc[split_index:][\"Date\"].values,\n",
    "        \"Province\": df_mining.iloc[split_index:][\"Province\"].values,\n",
    "        \"Actual\": y_test.values,\n",
    "        \"Prediction\": y_pred,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Export dataframe\n",
    "mined_df.to_excel(mined_dataset_path, index=False)\n",
    "\n",
    "# Show scatter plot between actual data vs prediction\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(mined_df[\"Actual\"], mined_df[\"Prediction\"], alpha=0.6)\n",
    "max_val = max(mined_df[\"Actual\"].max(), mined_df[\"Prediction\"].max())\n",
    "min_val = min(mined_df[\"Actual\"].min(), mined_df[\"Prediction\"].min())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], \"r--\", linewidth=2)\n",
    "\n",
    "plt.title(\"Actual vs Prediction\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show tree visualization\n",
    "n_trees_to_view = 3\n",
    "\n",
    "for i in range(n_trees_to_view):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    tree.plot_tree(model.estimators_[i], \n",
    "                   feature_names=X.columns, \n",
    "                   filled=True, \n",
    "                   max_depth=2,\n",
    "                   fontsize=12)\n",
    "    plt.title(f\"Visualisasi Estimator (Pohon) ke-{i+1} dalam Random Forest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f11c7b",
   "metadata": {},
   "source": [
    "# Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe51ae",
   "metadata": {},
   "source": [
    "## Generate Sugar Price History Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged.copy()\n",
    "\n",
    "# Specify where output will be stored\n",
    "combined_path = Path(\"../output/plots/sugar_price_history.png\")\n",
    "province_dir = Path(\"../output/plots/province_price_history/\")\n",
    "combined_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "province_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# All provinces combined plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for prov, g in df.groupby(\"Province\"):\n",
    "    plt.plot(g[\"Date\"], g[\"Price\"], label=prov, linewidth=0.8, alpha=0.7)\n",
    "\n",
    "plt.title(\"Riwayat Harga Gula per Provinsi\")\n",
    "plt.xlabel(\"Tanggal\")\n",
    "plt.ylabel(\"Harga (Rp)\")\n",
    "plt.legend(\n",
    "    title=\"Provinsi\", fontsize=6, ncol=2, bbox_to_anchor=(1.05, 1), loc=\"upper left\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(combined_path)\n",
    "plt.close()\n",
    "\n",
    "# Each individual province plot\n",
    "for prov, g in df.groupby(\"Province\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(g[\"Date\"], g[\"Price\"], linewidth=1.5)\n",
    "\n",
    "    plt.title(f\"Riwayat Harga Gula - {prov}\")\n",
    "    plt.xlabel(\"Tanggal\")\n",
    "    plt.ylabel(\"Harga (Rp)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    file_name = prov.replace(\" \", \"_\").replace(\"/\", \"_\") + \".png\"\n",
    "    plt.savefig(province_dir / file_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b639fbd",
   "metadata": {},
   "source": [
    "## Generate PACF Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_transform.copy()\n",
    "y_full = df['Price'].astype(float)\n",
    "max_lag = 31\n",
    "\n",
    "# Plot PACF\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "plot_pacf(y_full, ax=ax, lags=max_lag, method=\"ywm\")    # Method Yule-Walker Modified\n",
    "ax.set_title(f\"PACF Harga Gula Pasir (1 s.d. {max_lag} lag)\")\n",
    "ax.set_xlabel(\"Lag\")\n",
    "ax.set_ylabel(\"Partial Autocorrelation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create PACF value as numeric + confidence interval\n",
    "pacf_vals, confint = pacf(\n",
    "    y_full,\n",
    "    nlags=max_lag,\n",
    "    alpha=0.05, # 95% confidence interval\n",
    "    method=\"ywm\"\n",
    ")\n",
    "\n",
    "# Find significant lags\n",
    "significant_lags = [\n",
    "    lag for lag in range(1, len(pacf_vals))\n",
    "    if (confint[lag, 0] > 0) or (confint[lag, 1] < 0)\n",
    "]\n",
    "\n",
    "print(\"Lag yang signifikan menurut PACF:\", significant_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547a8d2",
   "metadata": {},
   "source": [
    "## Compare Between Actual and Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e613014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined target range\n",
    "horizon = 180\n",
    "max_date = df_mining[\"Date\"].max()\n",
    "range_target = max_date - pd.Timedelta(days=horizon)\n",
    "\n",
    "# Specify where outputs will be stored\n",
    "output_dir = Path(\"../output/xlsx\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / f\"rfr_{horizon}days_comparison.xlsx\"\n",
    "compare_plot_dir = Path(\"../output/plots/compare\")\n",
    "compare_plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split train / test by year\n",
    "train_mask = df_mining[\"Date\"] < range_target\n",
    "test_mask = df_mining[\"Date\"] >= range_target\n",
    "\n",
    "# Check if data available for train and test\n",
    "if not train_mask.any():\n",
    "    raise ValueError(f\"Tidak ada data {horizon} hari untuk training.\")\n",
    "if not test_mask.any():\n",
    "    raise ValueError(f\"Tidak ada data {horizon} hari untuk pengujian.\")\n",
    "\n",
    "# Sort dataframe by Date\n",
    "df_train = df_mining[train_mask].sort_values([\"Date\", \"Province\"])\n",
    "df_test = df_mining[test_mask].sort_values([\"Date\", \"Province\"])\n",
    "\n",
    "# Create X, y train and test\n",
    "X_train = df_train[FEATURE_COLS]\n",
    "y_train = df_train[TARGET_COL]\n",
    "\n",
    "X_test = df_test[FEATURE_COLS]\n",
    "y_test = df_test[TARGET_COL]\n",
    "\n",
    "# Create Random Forest Regression Model\n",
    "model = RandomForestRegressor(**RFR_PARAMS)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate Model\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"=== Comparison for {horizon} days ===\")\n",
    "print(f\"Train data up to: {df_train['Date'].max().date()}\")\n",
    "print(f\"RMSE : {rmse:,.2f}\")\n",
    "print(f\"MAPE : {mape:.2f}%\")\n",
    "\n",
    "# Export result\n",
    "result_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": df_test[\"Date\"].values,\n",
    "        \"Province\": df_test[\"Province\"].values,\n",
    "        \"Actual\": y_test.values,\n",
    "        \"Prediction\": y_pred,\n",
    "    }\n",
    ")\n",
    "\n",
    "result_df.to_excel(output_path, index=False)\n",
    "\n",
    "# Plot comparison result for each province\n",
    "for prov in sorted(result_df[\"Province\"].unique()):\n",
    "    sub = result_df[result_df[\"Province\"] == prov][[\"Date\", \"Actual\", \"Prediction\"]]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    ax.plot(sub[\"Date\"], sub[\"Actual\"], label=\"Actual\", linewidth=1.2)\n",
    "    ax.plot(sub[\"Date\"], sub[\"Prediction\"], label=\"Prediction\", linewidth=1.2)\n",
    "\n",
    "    ax.set_title(f\"Riwayat Harga Aktual vs Prediksi ({prov} - {horizon} hari)\")\n",
    "    ax.set_xlabel(\"Tanggal\")\n",
    "    ax.set_ylabel(\"Harga (Rp)\")\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    safe_filename = prov.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "    fig.savefig(compare_plot_dir / f\"{safe_filename}.png\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895227b",
   "metadata": {},
   "source": [
    "## Forecast Future Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_transform.copy()\n",
    "\n",
    "X = df[FEATURE_COLS]\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Create Random Forest Regression Model\n",
    "model = RandomForestRegressor(**RFR_PARAMS)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Specify how much data should be predicted\n",
    "horizon = 180\n",
    "\n",
    "forecast_results = []\n",
    "provinces = df[\"Province\"].unique()\n",
    "\n",
    "# Specify where forecast output will be stored\n",
    "forecast_plot_dir = Path(\"../output/plots/forecast/\")\n",
    "forecast_plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "forecast_output_path = Path(\"../output/xlsx/rfr_forecast_future_data.xlsx\")\n",
    "forecast_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Forecast loop each province\n",
    "for prov in provinces:\n",
    "    g = df[df[\"Province\"] == prov].copy()\n",
    "    g = g.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    last_date = g[\"Date\"].max()\n",
    "    last_rows = g.tail(30).copy()\n",
    "\n",
    "    future_rows = []\n",
    "\n",
    "    prov_id = g.iloc[-1][\"Province_id\"]\n",
    "\n",
    "    # Forecast loop until specified horizon\n",
    "    for i in range(1, horizon + 1):\n",
    "        next_date = last_date + pd.Timedelta(days=i)\n",
    "        \n",
    "        # Time Based Features\n",
    "        month = next_date.month\n",
    "        year  = next_date.year\n",
    "\n",
    "        # Lag Features\n",
    "        lag_1  = last_rows.iloc[-1][\"Price\"]\n",
    "        lag_14  = last_rows.iloc[-14][\"Price\"]  if len(last_rows) >= 14  else lag_1\n",
    "\n",
    "        # Eid Features\n",
    "        gdate = Gregorian(next_date.year, next_date.month, next_date.day)\n",
    "        h = gdate.to_hijri()\n",
    "\n",
    "        # 1 Syawal (Idul Fitri)\n",
    "        eid_h = Hijri(h.year, 10, 1)\n",
    "        eid_g = eid_h.to_gregorian()\n",
    "        eid_date = pd.Timestamp(eid_g.year, eid_g.month, eid_g.day)\n",
    "\n",
    "        delta = (next_date.normalize() - eid_date).days\n",
    "\n",
    "        before_eid = 1 if -7 <= delta <= -1 else 0\n",
    "        eid        = 1 if delta == 0 else 0\n",
    "        after_eid  = 1 if 1 <= delta <= 6 else 0\n",
    "\n",
    "        # Arrange future data features\n",
    "        X_future = pd.DataFrame({\n",
    "            \"Province_id\": [prov_id],\n",
    "            \"lag_1\": [lag_1],\n",
    "            \"lag_14\": [lag_14],\n",
    "            \"before_eid\": [before_eid],\n",
    "            \"eid\": [eid],\n",
    "            \"after_eid\": [after_eid],\n",
    "            \"month\": [month],\n",
    "            \"year\": [year]\n",
    "        })\n",
    "\n",
    "        predicted_price = model.predict(X_future)[0]\n",
    "\n",
    "        forecast_results.append({\n",
    "            \"Date\": next_date,\n",
    "            \"Province\": prov,\n",
    "            \"Prediction\": round(predicted_price),\n",
    "        })\n",
    "\n",
    "        future_rows.append({\n",
    "            \"Date\": next_date,\n",
    "            \"Price\": predicted_price\n",
    "        })\n",
    "\n",
    "        # Update series\n",
    "        last_rows = pd.concat([\n",
    "            last_rows,\n",
    "            pd.DataFrame([{\n",
    "                \"Date\": next_date,\n",
    "                \"Price\": predicted_price,\n",
    "                \"Province\": prov,\n",
    "                \"Province_id\": prov_id,\n",
    "            }])\n",
    "        ], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Plot\n",
    "    future_df = pd.DataFrame(future_rows)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(g[\"Date\"], g[\"Price\"], label=\"Historical\", linewidth=1.5)\n",
    "    plt.plot(future_df[\"Date\"], future_df[\"Price\"], label=f\"Forecast ({horizon} days)\", linewidth=1.5)\n",
    "\n",
    "    plt.title(f\"Historical + Forecast Harga Gula - {prov}\")\n",
    "    plt.xlabel(\"Tanggal\")\n",
    "    plt.ylabel(\"Harga (Rp)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    file_name = prov.replace(\" \", \"_\").replace(\"/\", \"_\") + \"_forecast.png\"\n",
    "    plt.savefig(forecast_plot_dir / file_name, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# Export Forecast Dataframe\n",
    "df_forecast = pd.DataFrame(forecast_results)\n",
    "df_forecast = df_forecast.sort_values([\"Province\", \"Date\"]).reset_index(drop=True)\n",
    "df_forecast.to_excel(forecast_output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
